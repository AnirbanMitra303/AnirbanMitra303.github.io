I"›<!--<div style="text-align: justify" style="font-size:1.2em">-->
<p>My first encounter with Wigner‚Äôs name was when I was suggested to read the Wigner-Ville distribution and Gabor transform during my brief work on a signal processing project three years back. The project was short and I did not care to make any in-depth venture into Wigner‚Äôs work. A couple years after that I had started to read on random matrix perturbations when I started come across terms such as <strong>Wigner matrix</strong>, <strong>Wigner‚Äôs surmise</strong>, <strong>Wigner semi-circle law</strong>, and so on. Now this piqued my interest because I had heard of Wigner before. Upon looking up the Wikipedia page I found that Eugene Wigner was a physicist (well, primarily) and a Nobel laureate (1963)! <br />
Only recently I had the pleasure of coming across the <em>The Oxford Handbook of Random Matrix Theory</em>, and I am so very thankful to myself for digging on this subject in our library catalogue. This blog (hopefully a series) will be based on this book and few external supportive research/reading that I conduct. So, as I read along, start to understand the origins and implications  of <strong>Random Matrix Theory</strong> (RMT) (and get my mind blown), I will continue writing on this. This is such an ocean of knowledge, I will be extremely happy if I can digest even a drop of it.<br /><br />
It is indeed very hard (as a novice) to decide from where to begin. Let me start by defining some matrices and matrix ensembles. Let us say \(\mathbf{W}\) is an \(n\)-dimensional symmetric matrix with the \((i,j)\)-th entry denoted by \(W_{ij}\). \(\mathbf{W}\) is a Wigner matrix if it satisfies the following conditions:</p>
<ul>
    <li style="font-size:1.2em">The entries are independent upto symmetry. That is $W_{ij}$ are independent for all $1 \le i\le j \le n$.</li>
    <li style="font-size:1.2em">The diagonal entries are independent and identically distributed (i.i.d.) and the entries in the lower triangle are also i.i.d and also independent of the diagonal entries.</li>
    <li style="font-size:1.2em">The expected values of the squares of entries are finite, i.e. $\mathbb{E}\left(W_{ij}^{2}\right) &lt; \infty$.</li>
</ul>
<p>Next, let us learn about some matrix ensembles, more specifically Gaussian ensembles. The name \textit{Gaussian ensemble} suggest that it has something to do with the Gaussian or Normal distribution. We will see that in a bit, but why ‚Äúensembles‚Äù? Well, to study asymptotics (or convergence) we most obviously need the number of ‚Äúvirtual‚Äù copies to be large or in this case we need the dimension of matrix to be large. The entries of a Gaussian ensemble, \(\mathbf{H}_{n}\) (the subscript \(n\) denotes the dimension of the matrix as well as hinting at the fact that a growing \(n\) is used to study asymptotics), are independent upto symmetry and have the following probability density:</p>

\[\begin{align*}
  f(\mathbf{H}) = \frac{1}{Z_{GE}}\exp\left(-\frac{n\beta}{4}\operatorname{tr}(\mathbf{H}^{2})\right)
\end{align*}\]

<p class="text-justify">\(Z_{GE}\) is the normalizing constant which depends on the type of ensemble. Three types of ensembles are typically considered: Gaussian Orthogonal Ensemble (GOE), Gaussian Unitary Ensemble (GUE) and Gaussian Simplectic Ensemble (GSE). So the density is essentially proportional to an exponent. Now the exponent depends on the term \(\beta\), called the Dyson index which takes values \(1\) for GOE, \(2\) for GUE and \(4\) for GSE.</p>
<!--</div>-->
:ET